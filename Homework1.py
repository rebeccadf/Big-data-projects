# -*- coding: utf-8 -*-
"""Copia di Mappy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZ5Sdhl3RSMRLpsvQyFsF8KhoYgTe_WA
"""

from pyspark import SparkContext, SparkConf
import sys
import os
import random as rand

def invert(data, S):
  word = data.split(',')
  return [(word[6], word[1])] #(customer ID, product ID) pairs

def invert2(pairs):
  products = pairs[1].split(',')
  dictionary = {} #dictionary of prod id, customer id 
  for prod_id in range(len(products)):
    if products[prod_id] not in dictionary.keys(): #to obtain distinct productIDs for each customer
      dictionary[products[prod_id]] = pairs[0]
  
  return list(dictionary.items())

def filter_data(data, S):
  word = data.split(',')
  if int(word[3]) > 0: #filter by quantity > 0
    if word[7] == S: #filter by country
      return data
    elif S == "all": #no condition on country
      return data

def create_pairs(rawData, S):
  productCustomer = (rawData.filter(lambda x: filter_data(x, S)) #filter the RDD by the conditions
                     .flatMap(lambda x: invert(x, S)) #it takes each transation and returns a RDD of pairs (customerID, productID)
                     .reduceByKey(lambda x, y: x+','+y) #merge the values of all customerID keys (to remove duplicate customer keys) by applying string concatenation on their values
                     .flatMap(lambda x: invert2(x))) #reverse the paired RDD: from (customerID, productID) -> (productID, customerID)
  
  return productCustomer

def count_for_partition(tuples):
  count = {}
  for tup in tuples: #(productID, customerID)
    if tup[0] not in count.keys():
      count[tup[0]] = 1
    else:
      count[tup[0]] += 1
  return list(count.items())

def popularity(pairs):
  productPopularity1 = (pairs.mapPartitions(count_for_partition) #it takes each partition and apply a function that returns pairs (productID, n of distinct customers for that product)
                        .groupByKey() #it groups the RDD by productIDs
                        .mapValues(lambda x: sum(x))) #it sums the count of customers obtained by each partition keeping the product id keys fixed
  return productPopularity1

def popularity2(pairs):
  productPopularity2 = (pairs.map(lambda x: (x[0],1)) #it maps every productID with a 1
                        .reduceByKey(lambda x, y: x+y)) #for every productID it sums the list of ones obtained by the map function
  return productPopularity2

def most_popular(pairs, H):
  list_pop = (pairs.map(lambda x: (x[1], x[0])) #reversing the order of the pairs: from (productID, popularity) -> (popularity, productID)
              .sortByKey(ascending= False)) #ordering the RDD by most popular product
  if H == 0:
    list_pop = list_pop.collect() #convert the RDD into a list
    list_pop.sort(key= lambda a: a[1]) #sort the list by lexicographical order of productID

  elif H > 0:
    list_pop = list_pop.take(H) #retrieve into a list only the H most popular products
    list_pop.sort(key= lambda a: a[0], reverse= True) #order the list by highest values of popularity 
  
  return [el[::-1] for el in list_pop] #reverse again the order of the pairs to obtain a list of (productID, popularity) tuples

def main():

    # CHECKING NUMBER OF CMD LINE PARAMTERS
    assert len(sys.argv) == 5, "Usage: python GxxxHW1.py <K> <H> <string_S> <file_name>"

    # SPARK SETUP
    conf = SparkConf().setAppName('WordCountExample').setMaster("local[*]")
    sc = SparkContext(conf=conf)

    # INPUT READING

    # Read number of partitions
    K = sys.argv[1]
    assert K.isdigit(), "K must be an integer"
    K = int(K)
    
    # Read H which is a number that will be used to retrieve the H products with highest popularity
    H = sys.argv[2]
    assert H.isdigit(), "H must be an integer"
    H = int(H)

    #Read a string S for the country
    S = sys.argv[3]
    assert isinstance(S, str), "S must be a string"
    S = str(S)
    
    #1. Read input file and subdivide it into K random partitions
    data_path = sys.argv[4]
    assert os.path.isfile(data_path), "File or folder not found"
    rawData = sc.textFile(data_path,minPartitions=K).cache()
    rawData.repartition(numPartitions=K)
    #Counting the number of rows 
    numrows = rawData.count()
    print("Number of rows = ", numrows)
    
    #2. Transform rawData into an RDD of (String,Integer) pairs called productCustomer
    productCustomer = create_pairs(rawData, S)
    print("Product-Customer Pairs = ", productCustomer.count())
    
    #3. Uses the mapPartitions method to transform productCustomer into an RDD called productPopularity1 
    productPopularity1 = popularity(productCustomer)

    #4. productPopularity2 with map + reduceByKey
    productPopularity2 = popularity2(productCustomer)

    #5. Print the H products with highest Popularity
    list_popularity = most_popular(productPopularity1, H)
    if H>0:
      print(f'Top {H} Products and their Popularities')
    elif H == 0:
      print("productPopularity1: ")
    for tup in list_popularity:
      print(f'Product {tup[0]} Popularity {tup[1]};', end=' ') #6. if H==0 debug purposes
    
    if H==0: #6. Debug purposes
      list_popularity2 = most_popular(productPopularity2, H)
      print("productPopularity2:")
      for tup in list_popularity:
        print(f'Product {tup[0]} Popularity {tup[1]};', end=' ')

if __name__ == "__main__":
	main()